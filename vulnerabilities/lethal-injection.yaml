title: Lethal Injection
slug: lethal-injection
cves:
affectedPlatforms:
- Azure
affectedServices:
- Health Bot
image: 
  https://images.unsplash.com/photo-1611690828749-66c846dbd1b4?q=80&w=2070&auto=format&fit=crop&ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D
severity: Critical
discoveredBy:
  name: Yanir Tsarimi
  org: Breachproof
  domain: breachproof.net
  twitter: yanir_
publishedAt: 2024/05/07
disclosedAt:
exploitabilityPeriod:
knownITWExploitation: false
summary: |
  Multiple vulnerabilities were uncovered in Azure Health Bot service, Microsoft's health chatbot platform.
  These could have potentially exposed sensitive user data and granted attackers extensive control, allowing
  unrestricted code execution as root on the bot backend, unrestricted access to authentication secrets &
  integration auth providers, unrestricted memory read in the bot backend, exposing sensitive secrets,
  allowing cross-tenant data access and unrestricted deletion of other tenants' public resources.
  These issues stemmed from various bugs related to URL sanitization, shared compute, and sandboxing.
  Following disclosure, Microsoft changed the service architecture to run a completely separate ACI
  instance per customer, thereby mitigating future sandbox escapes, and changed the sandboxing from
  vm2 to the isolated-vm library (which uses V8 isolates).
manualRemediation: |
  None required
detectionMethods:
contributor: https://github.com/korniko98
references:
- https://www.breachproof.net/blog/lethal-injection-how-we-hacked-microsoft-ai-chat-bot
- https://twitter.com/Yanir_/status/1787927285443494137
entryStatus: Finalized
